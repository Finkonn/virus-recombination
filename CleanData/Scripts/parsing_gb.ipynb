{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get records from .gb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Records/FMDV_records.gb'\n",
    "genbank_records = SeqIO.parse(file_path, 'gb')\n",
    "genbank_records = list(genbank_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty lists for storing necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosts = []\n",
    "collection_dates = []\n",
    "countries = []\n",
    "isolation_sources = []\n",
    "isolates = []\n",
    "strains = []\n",
    "serotypes = []\n",
    "record_ids = []\n",
    "references = []\n",
    "number_of_records = 0\n",
    "CDS_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 1870\n",
      "Number of CDS: 1870\n",
      "Hosts: ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Bubalus bubalis']\n",
      "Collection Dates: ['Unknown', 'Unknown', 'Unknown', 'Unknown', '01-Dec-2013']\n",
      "Countries: ['Spain', 'Spain', 'Spain:Olot', 'Unknown', 'India']\n",
      "Isolation Sources: ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown']\n",
      "Isolates: ['rp146', 'rp99', 'C-s8c1', 'Unknown', 'FMDV/Goat/India/2013/Shahjadpur/P0']\n",
      "Strains: ['C', 'C', 'C', 'A10-61', 'Unknown']\n",
      "Serotypes: ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'O']\n",
      "Record IDs: ['AJ133359', 'AJ133358', 'AJ133357', 'X00429', 'OK422491']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The function checks if the qualifier is a list and returns the appropriate value.\n",
    "If the qualifier is a list, its first element is returned.\n",
    "If the qualifier is not a list, it is returned as is.\n",
    "If the qualifier is not found by the key, the string 'Unknown' is returned. \n",
    "\"\"\"\n",
    "def parse_qualifier(qualifier, key):\n",
    "    value = qualifier.get(key, 'Unknown')\n",
    "    return value if not isinstance(value, list) else value[0]\n",
    "\n",
    "def merge_values(values):\n",
    "    #Merges multiple values into one by prioritizing non-'Unknown' values.\n",
    "    for value in values:\n",
    "        if value != 'Unknown':\n",
    "            return value\n",
    "    return 'Unknown'\n",
    "\n",
    "# The function applies parse_qualifier to the necessary qualifiers.\n",
    "def extract_data_from_record(record):\n",
    "    features = record.features\n",
    "    source_feature_list = [feature for feature in features if feature.type == 'source']\n",
    "\n",
    "    # Dictionary to store merged values for each qualifier\n",
    "    merged_values = {\n",
    "        'host': [],\n",
    "        'collection_date': [],\n",
    "        'country': [],\n",
    "        'isolation_source': [],\n",
    "        'isolate': [],\n",
    "        'strain': [],\n",
    "        'serotype': []\n",
    "    }\n",
    "\n",
    "    # Iterate through 'source' features\n",
    "    for source_feature in source_feature_list:\n",
    "        qual_dict = source_feature.qualifiers\n",
    "\n",
    "        # Iterate through qualifiers and merge values\n",
    "        for key in merged_values.keys():\n",
    "            merged_values[key].append(parse_qualifier(qual_dict, key))\n",
    "\n",
    "    # Append merged values to respective lists\n",
    "    hosts.append(merge_values(merged_values['host']))\n",
    "    collection_dates.append(merge_values(merged_values['collection_date']))\n",
    "    countries.append(merge_values(merged_values['country']))\n",
    "    isolation_sources.append(merge_values(merged_values['isolation_source']))\n",
    "    isolates.append(merge_values(merged_values['isolate']))\n",
    "    strains.append(merge_values(merged_values['strain']))\n",
    "    serotypes.append(merge_values(merged_values['serotype']))\n",
    "\n",
    "    # Extract references\n",
    "    for reference in record.annotations[\"references\"]:\n",
    "        references.append(reference.title)\n",
    "        break\n",
    "\n",
    "    # Extract record ids    \n",
    "    record_ids.append(record.name)\n",
    "\n",
    "# Execute the function for each record in the .gb file.\n",
    "for record in genbank_records:\n",
    "    extract_data_from_record(record)\n",
    "    CDS_count += sum(1 for feature in record.features if feature.type == 'CDS')\n",
    "    number_of_records += 1\n",
    "\n",
    "print(\"Number of records:\", number_of_records)\n",
    "print(\"Number of CDS:\", CDS_count)\n",
    "print(\"Hosts:\", hosts[:5])\n",
    "print(\"Collection Dates:\", collection_dates[:5])\n",
    "print(\"Countries:\", countries[:5])\n",
    "print(\"Isolation Sources:\", isolation_sources[:5])\n",
    "print(\"Isolates:\", isolates[:5])\n",
    "print(\"Strains:\", strains[:5])\n",
    "print(\"Serotypes:\", serotypes[:5])\n",
    "print(\"Record IDs:\", record_ids[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading map.csv files\n",
    "def read_csv(file_name):\n",
    "    with open(file_name) as csvfile:\n",
    "        reader = csv.DictReader(csvfile,\n",
    "                                delimiter=\",\", \n",
    "                                fieldnames=[\"base\", \"new\"])\n",
    "        result = {}\n",
    "        \n",
    "        for row in reader:\n",
    "            base_value = row[\"base\"].strip()\n",
    "            new_value = row[\"new\"].strip().rstrip(';')\n",
    "            result[base_value] = new_value\n",
    "        csvfile.close()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function maps values from a list to values in a CSV file.\n",
    "def map_qualifiers(qualifier, csv_file):\n",
    "    data_dict = read_csv(csv_file)\n",
    "    feature_map_comp = [(re.compile(key), value) for key, value in data_dict.items()]\n",
    "    mapped_values = []\n",
    "    for name in qualifier:\n",
    "        for regex, new_name in feature_map_comp:\n",
    "            match = regex.search(name)\n",
    "            if match:\n",
    "                mapped_values.append(new_name)\n",
    "                break\n",
    "        else:\n",
    "            mapped_values.append(name)\n",
    "    return mapped_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping hosts\n",
    "hosts_mapped = map_qualifiers(hosts, '../Maps/host_map.csv')\n",
    "\n",
    "# Mapping countries\n",
    "countries_mapped = map_qualifiers(countries, '../Maps/country_map.csv')\n",
    "\n",
    "# Converting all dates to years\n",
    "collection_years = ['Unknown' if date == 'Unknown' else (date[-4:] if date[-4:].isdigit() else date[:4]) for date in collection_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function prints unique values and their count in the list\n",
    "def print_sorted_counts(items):\n",
    "    count_dict = {}\n",
    "    for item in items:    \n",
    "        count_dict[item] = count_dict.get(item, 0) + 1\n",
    "    \n",
    "    for key, value in sorted(count_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save qualifier values to a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'GenBankAccession': record_ids,\n",
    "    'Country': countries_mapped,\n",
    "    'Host': hosts_mapped,\n",
    "    'CollectionDate': collection_years,\n",
    "    'Serotype': serotypes,\n",
    "    'Strain': strains,\n",
    "    'Isolate': isolates,\n",
    "    'IsolationSource': isolation_sources\n",
    "})\n",
    "df.to_csv('../qualifiers_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file with sources to remove\n",
    "def read_exclusion_criteria(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        exclusion_criteria = [line.strip() for line in file]\n",
    "    return exclusion_criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function excludes genbank records based on isolation source and reference exclusion criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_records(genbank_records, isolation_exclusion_file, reference_exclusion_file, output_file):\n",
    "    isolation_exclusion_criteria = read_exclusion_criteria(isolation_exclusion_file)\n",
    "    reference_exclusion_criteria = read_exclusion_criteria(reference_exclusion_file)\n",
    "\n",
    "    filtered_records = [record for i, record in enumerate(genbank_records)\n",
    "                        if not any(re.search(criteria, isolation_sources[i]) for criteria in isolation_exclusion_criteria)\n",
    "                        and not any(re.search(criteria, references[i]) for criteria in reference_exclusion_criteria)]\n",
    "\n",
    "    with open(output_file, 'w') as genbank_file:\n",
    "        SeqIO.write(filtered_records, genbank_file, 'gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_records(genbank_records, '../Maps/isolation_sources_to_remove.txt', '../Maps/references_to_remove.txt', '../Records/filtered_gb_records.gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hosts = []\n",
    "filtered_collection_dates = []\n",
    "filtered_countries = []\n",
    "filtered_isolation_sources = []\n",
    "filtered_isolates = []\n",
    "filtered_strains = []\n",
    "filtered_serotypes = []\n",
    "filtered_record_ids = []\n",
    "filtered_references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_filtered_record(record):\n",
    "    features = record.features\n",
    "    source_feature_list = [feature for feature in features if feature.type == 'source']\n",
    "\n",
    "    # Dictionary to store merged values for each qualifier\n",
    "    merged_values = {\n",
    "        'host': [],\n",
    "        'collection_date': [],\n",
    "        'country': [],\n",
    "        'isolation_source': [],\n",
    "        'isolate': [],\n",
    "        'strain': [],\n",
    "        'serotype': []\n",
    "    }\n",
    "\n",
    "    # Iterate through 'source' features\n",
    "    for source_feature in source_feature_list:\n",
    "        qual_dict = source_feature.qualifiers\n",
    "\n",
    "        # Iterate through qualifiers and merge values\n",
    "        for key in merged_values.keys():\n",
    "            merged_values[key].append(parse_qualifier(qual_dict, key))\n",
    "\n",
    "    # Append merged values to respective lists\n",
    "    filtered_hosts.append(merge_values(merged_values['host']))\n",
    "    filtered_collection_dates.append(merge_values(merged_values['collection_date']))\n",
    "    filtered_countries.append(merge_values(merged_values['country']))\n",
    "    filtered_isolation_sources.append(merge_values(merged_values['isolation_source']))\n",
    "    filtered_isolates.append(merge_values(merged_values['isolate']))\n",
    "    filtered_strains.append(merge_values(merged_values['strain']))\n",
    "    filtered_serotypes.append(merge_values(merged_values['serotype']))\n",
    "\n",
    "    # Extract references\n",
    "    for reference in record.annotations[\"references\"]:\n",
    "        filtered_references.append(reference.title)\n",
    "        break\n",
    "\n",
    "    # Extract record ids    \n",
    "    filtered_record_ids.append(record.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_genbank_records = SeqIO.parse('../Records/filtered_gb_records.gb', 'gb')\n",
    "filtered_genbank_records = list(filtered_genbank_records)\n",
    "for record in filtered_genbank_records:\n",
    "    extract_data_from_filtered_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping hosts\n",
    "filtered_hosts_mapped = map_qualifiers(filtered_hosts, '../Maps/host_map.csv')\n",
    "\n",
    "# Mapping countries\n",
    "filtered_countries_mapped = map_qualifiers(filtered_countries, '../Maps/country_map.csv')\n",
    "\n",
    "# Converting all dates to years\n",
    "filtered_collection_years = ['Unknown' if date == 'Unknown' else (date[-4:] if date[-4:].isdigit() else date[:4]) for date in filtered_collection_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'GenBankAccession': filtered_record_ids,\n",
    "    'Country':          filtered_countries_mapped,\n",
    "    'Host':             filtered_hosts_mapped,\n",
    "    'CollectionDate':   filtered_collection_years,\n",
    "    'Serotype':         filtered_serotypes,\n",
    "    'Strain':           filtered_strains,\n",
    "    'Isolate':          filtered_isolates,\n",
    "    'IsolationSource':  filtered_isolation_sources\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Serotype': 'Annotated serotype'})\n",
    "\n",
    "records = SeqIO.parse('../aln_repo/VP1_genotyped.fasta', \"fasta\")\n",
    "extracted_serotypes = []\n",
    "topotypes = []\n",
    "\n",
    "for record in records:\n",
    "    extracted_serotype = record.id.split('/')[-1]\n",
    "    \n",
    "    if extracted_serotype == \"O-CATHAY\" or extracted_serotype == \"Pan-Asia-O\":\n",
    "        extracted_serotypes.append(\"O\")\n",
    "        topotypes.append(extracted_serotype)\n",
    "    else:\n",
    "        extracted_serotypes.append(extracted_serotype)\n",
    "        topotypes.append('Unknown')\n",
    "\n",
    "df['Genotyped serotype'] = extracted_serotypes\n",
    "df['Topotype'] = topotypes\n",
    "\n",
    "df.to_csv('../qualifiers_table_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function extracts CDS sequences and creates a FASTA file with headers in the format >GenbankAC/country/host/year/serotype.\n",
    "\n",
    "For this purpose, mapped lists are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta_from_genbank(genbank_records, hosts, collection_dates, countries, serotypes, record_ids, isolation_exclusion_file, reference_exclusion_file, output_file):\n",
    "    isolation_exclusion_criteria = read_exclusion_criteria(isolation_exclusion_file)\n",
    "    reference_exclusion_criteria = read_exclusion_criteria(reference_exclusion_file)\n",
    "    with open(output_file, 'w') as fasta_file:\n",
    "        for i, record in enumerate(genbank_records):  \n",
    "\n",
    "            isolation_source = isolation_sources[i]\n",
    "            reference = references[i]\n",
    "\n",
    "            # Check if the isolation source matches any exclusion criteria\n",
    "            if any(re.search(criteria, isolation_source) for criteria in isolation_exclusion_criteria):\n",
    "                continue  # Skip this record\n",
    "            \n",
    "            # Check if the reference matches any exclusion criteria\n",
    "            if any(re.search(criteria, reference) for criteria in reference_exclusion_criteria):\n",
    "                continue  # Skip this record\n",
    "            \n",
    "            # Constructing the header\n",
    "            header = f\">{record_ids[i].replace(' ', '-')}/{countries[i].replace(' ', '-')}/{hosts[i].replace(' ', '-')}/{collection_dates[i].replace(' ', '-')}/{serotypes[i].replace(' ', '-')}\"\n",
    "            fasta_file.write(header + '\\n')\n",
    "\n",
    "            cds_list = []\n",
    "            # Getting the coordinates of the coding sequence/sequences\n",
    "            for feature in record.features:\n",
    "                if feature.type == 'CDS':\n",
    "                    # Check if product qualifier is 'polyprotein'\n",
    "                    if 'product' in feature.qualifiers:\n",
    "                        product_value = feature.qualifiers['product'][0]\n",
    "                        pattern = r'(?i)(polyprotein|poylprotein|polyprotein precursor|polypeptide)'\n",
    "                        if re.match(pattern, product_value): \n",
    "                        \n",
    "                            cds_start = feature.location.start.position\n",
    "                            cds_end = feature.location.end.position\n",
    "                            \n",
    "                            cds_sequence = record.seq[cds_start:cds_end]\n",
    "                            cds_list.append(str(cds_sequence))\n",
    "                            full_cds = ''.join(cds_list)\n",
    "\n",
    "            # Writing the sequence, moving to a new line every 70 characters\n",
    "            for i in range(0, len(full_cds), 70):\n",
    "                fasta_file.write(str(full_cds[i:i+70]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_fasta_from_genbank(genbank_records, hosts_mapped, collection_years, countries_mapped, serotypes, record_ids, '../Maps/isolation_sources_to_remove.txt', '../Maps/references_to_remove.txt', '../Sequences/CDS_all.fasta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
